{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= import ===================================\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# ======================== CFG ======================================\n",
    "\n",
    "class CFG:\n",
    "\n",
    "    episode = 500\n",
    "    MC_alpha = [0.01, 0.02, 0.03, 0.04]\n",
    "    TD_alpha = [0.05, 0.1, 0.15]\n",
    "    target = [1/6, 2/6, 3/6, 4/6, 5/6]\n",
    "    run = 500\n",
    "    garma = 1.0\n",
    "\n",
    "# ======================== init ======================================\n",
    "\n",
    "class game():\n",
    "    def __init__(self, alpha, TypeAlgo):\n",
    "        self.value_function = np.full((7), 0.5)\n",
    "        self.policy = np.full((7,2), 0.5)\n",
    "        self.Left = -1\n",
    "        self.Right = 1\n",
    "        self.Stop = 0\n",
    "        self.actions = [self.Left, self.Right]\n",
    "        self.FirstEnd = 0\n",
    "        self.LastEnd = 6\n",
    "        self.alpha = alpha\n",
    "        self.TypeAlgo = TypeAlgo\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        if (self.TypeAlgo == 'MC'):\n",
    "            return self.step_MC()\n",
    "        if (self.TypeAlgo == 'TD'):\n",
    "            return self.step_TD()\n",
    "\n",
    "    def step_TD(self):\n",
    "        his = []\n",
    "        cur = 3\n",
    "        reward = 0\n",
    "\n",
    "        while(cur != self.FirstEnd and cur != self.LastEnd):\n",
    "            action = np.random.choice(self.actions, p=self.policy[cur, :])\n",
    "            state_reward = 0\n",
    "            his.append((cur, action, state_reward))\n",
    "            cur += action\n",
    "            reward += state_reward\n",
    "        if cur == self.LastEnd:\n",
    "            state_reward = 1\n",
    "        else:\n",
    "            state_reward = 0\n",
    "        his.append((cur, self.Stop, state_reward))\n",
    "\n",
    "        return his, reward\n",
    "\n",
    "    def step_MC(self):\n",
    "        his = []\n",
    "        cur = 3\n",
    "        reward = 0\n",
    "\n",
    "        while(cur != self.FirstEnd and cur != self.LastEnd):\n",
    "            action = np.random.choice(self.actions, p=self.policy[cur, :])\n",
    "            if (cur + action == self.LastEnd):\n",
    "                state_reward = 1\n",
    "            else:\n",
    "                state_reward = 0\n",
    "            his.append((cur, action, state_reward))\n",
    "            cur += action\n",
    "            reward += state_reward\n",
    "\n",
    "        return his, reward\n",
    "\n",
    "    def update(self, state, action, state_reward, reward):\n",
    "        if (self.TypeAlgo == 'MC'):\n",
    "            return self.update_MC(state, reward)\n",
    "        if (self.TypeAlgo == 'TD'):\n",
    "            return self.update_TD(state, state + action, state_reward)\n",
    "\n",
    "    def update_TD(self, state, Nstate, state_reward):\n",
    "        up = self.alpha * (state_reward + CFG.garma * self.value_function[Nstate] * int(state != Nstate) - self.value_function[state])\n",
    "        self.value_function[state] += up\n",
    "        return up\n",
    "    \n",
    "    def update_MC(self, state, reward):\n",
    "        up = self.alpha * (reward - self.value_function[state])\n",
    "        self.value_function[state] += up\n",
    "        return up\n",
    "\n",
    "    def get_value_function(self):\n",
    "        return self.value_function\n",
    "\n",
    "# ======================= function ===================================\n",
    "\n",
    "def MC_evaluate(episode, alpha):\n",
    "    total_error = np.zeros((episode), dtype = float)\n",
    "    for _ in trange(CFG.run, desc = f'MC - {alpha}'):\n",
    "        env = game(alpha = alpha, TypeAlgo = 'MC')\n",
    "        error = []\n",
    "        for ep in range(episode):\n",
    "            env.reset()\n",
    "            his, reward = env.step()\n",
    "            for i in range(len(his)):\n",
    "                state, action, state_reward = his[i]\n",
    "                env.update(state, action, state_reward, reward)\n",
    "            error.append(np.sqrt(np.sum(np.power(env.get_value_function()[1:-1] - CFG.target, 2))/5.0))\n",
    "        error = np.asarray(error)\n",
    "        total_error += error\n",
    "    return total_error/CFG.run\n",
    "\n",
    "def TD_evaluate(episode, alpha, TDplot = False):\n",
    "    total_error = np.zeros((episode), dtype = float)\n",
    "    for r in trange(CFG.run, desc = f'TD - {alpha}'):\n",
    "        env = game(alpha = alpha, TypeAlgo = 'TD')\n",
    "        error = []\n",
    "        for ep in range(episode):\n",
    "            env.reset()\n",
    "            his, reward = env.step()\n",
    "            update = np.zeros((7), dtype = float)\n",
    "            for i in range(len(his)):\n",
    "                state, action, state_reward = his[i]\n",
    "                env.update(state, action, state_reward, reward)\n",
    "\n",
    "            env.value_function += update\n",
    "            error.append(np.sqrt(np.sum(np.power(env.get_value_function()[1:-1] - CFG.target, 2))/5.0))\n",
    "            if (r == 1 and TDplot):\n",
    "                if (ep == 0 or ep == 1 or ep == 10 or ep == 100 or ep == 200 or ep == 299):\n",
    "                    plt.plot(env.get_value_function()[1:-1], label = f'{ep}')\n",
    "        error = np.asarray(error)\n",
    "        total_error += error\n",
    "        if (r == 1 and TDplot):\n",
    "            plt.plot(CFG.target,label = 'true values')\n",
    "            plt.xticks([0,1,2,3,4],['A','B','C','D','E'])\n",
    "            plt.legend()\n",
    "            plt.savefig('figure_6_6/6_6.png')\n",
    "            plt.close()\n",
    "            return\n",
    "    \n",
    "    return total_error/CFG.run\n",
    "\n",
    "\n",
    "# ======================= main ========================================\n",
    "\n",
    "for alpha in CFG.MC_alpha:\n",
    "    error = MC_evaluate(CFG.episode, alpha)\n",
    "    plt.plot(error,'--', label = f'MC - {alpha}')\n",
    "\n",
    "for alpha in CFG.TD_alpha:\n",
    "    error = TD_evaluate(CFG.episode, alpha)\n",
    "    plt.plot(error, label = f'TD - {alpha}')\n",
    "plt.legend()\n",
    "plt.savefig('figure_6_7/6_7.png')\n",
    "plt.close()\n",
    "\n",
    "TD_evaluate(CFG.episode, 0.1, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
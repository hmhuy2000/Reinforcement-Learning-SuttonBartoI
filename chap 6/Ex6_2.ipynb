{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= import ===================================\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# ======================== CFG ======================================\n",
    "\n",
    "class CFG:\n",
    "\n",
    "    episode = 500\n",
    "    MC_alpha = [0.01, 0.02, 0.03, 0.04]\n",
    "    TD_alpha = [0.05, 0.1, 0.15]\n",
    "    target = [1/6, 2/6, 3/6, 4/6, 5/6]\n",
    "    run = 500\n",
    "    garma = 1.0\n",
    "\n",
    "# ======================== init ======================================\n",
    "\n",
    "class game():\n",
    "    def __init__(self, alpha, TypeAlgo):\n",
    "        self.value_function = np.full((7), 0.5)\n",
    "        self.policy = np.full((7,2), 0.5)\n",
    "        self.Left = -1\n",
    "        self.Right = 1\n",
    "        self.Stop = 0\n",
    "        self.actions = [self.Left, self.Right]\n",
    "        self.FirstEnd = 0\n",
    "        self.LastEnd = 6\n",
    "        self.alpha = alpha\n",
    "        self.TypeAlgo = TypeAlgo\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        if (self.TypeAlgo == 'MC'):\n",
    "            return self.step_MC()\n",
    "        if (self.TypeAlgo == 'TD'):\n",
    "            return self.step_TD()\n",
    "\n",
    "    def step_TD(self):\n",
    "        his = []\n",
    "        cur = 3\n",
    "        reward = 0\n",
    "\n",
    "        while(cur != self.FirstEnd and cur != self.LastEnd):\n",
    "            action = np.random.choice(self.actions, p=self.policy[cur, :])\n",
    "            state_reward = 0\n",
    "            his.append((cur, action, state_reward))\n",
    "            cur += action\n",
    "            reward += state_reward\n",
    "        if cur == self.LastEnd:\n",
    "            state_reward = 1\n",
    "        else:\n",
    "            state_reward = 0\n",
    "        his.append((cur, self.Stop, state_reward))\n",
    "\n",
    "        return his, reward\n",
    "\n",
    "    def step_MC(self):\n",
    "        his = []\n",
    "        cur = 3\n",
    "        reward = 0\n",
    "\n",
    "        while(cur != self.FirstEnd and cur != self.LastEnd):\n",
    "            action = np.random.choice(self.actions, p=self.policy[cur, :])\n",
    "            if (cur + action == self.LastEnd):\n",
    "                state_reward = 1\n",
    "            else:\n",
    "                state_reward = 0\n",
    "            his.append((cur, action, state_reward))\n",
    "            cur += action\n",
    "            reward += state_reward\n",
    "\n",
    "        return his, reward\n",
    "\n",
    "    def update(self, state, action, state_reward, reward):\n",
    "        if (self.TypeAlgo == 'MC'):\n",
    "            return self.update_MC(state, reward)\n",
    "        if (self.TypeAlgo == 'TD'):\n",
    "            return self.update_TD(state, state + action, state_reward)\n",
    "\n",
    "    def update_TD(self, state, Nstate, state_reward):\n",
    "        up = self.alpha * (state_reward + CFG.garma * self.value_function[Nstate] * int(state != Nstate) - self.value_function[state])\n",
    "        self.value_function[state] += up\n",
    "        return up\n",
    "    \n",
    "    def update_MC(self, state, reward):\n",
    "        up = self.alpha * (reward - self.value_function[state])\n",
    "        self.value_function[state] += up\n",
    "        return up\n",
    "\n",
    "    def get_value_function(self):\n",
    "        return self.value_function\n",
    "\n",
    "# ======================= function ===================================\n",
    "\n",
    "def MC_evaluate(episode, alpha):\n",
    "    total_error = np.zeros((episode), dtype = float)\n",
    "    for _ in trange(CFG.run, desc = f'MC - {alpha}'):\n",
    "        env = game(alpha = alpha, TypeAlgo = 'MC')\n",
    "        error = []\n",
    "        for ep in range(episode):\n",
    "            env.reset()\n",
    "            his, reward = env.step()\n",
    "            for i in range(len(his)):\n",
    "                state, action, state_reward = his[i]\n",
    "                env.update(state, action, state_reward, reward)\n",
    "            error.append(np.sqrt(np.sum(np.power(env.get_value_function()[1:-1] - CFG.target, 2))/5.0))\n",
    "        error = np.asarray(error)\n",
    "        total_error += error\n",
    "    return total_error/CFG.run\n",
    "\n",
    "def TD_evaluate(episode, alpha, TDplot = False):\n",
    "    total_error = np.zeros((episode), dtype = float)\n",
    "    for r in trange(CFG.run, desc = f'TD - {alpha}'):\n",
    "        env = game(alpha = alpha, TypeAlgo = 'TD')\n",
    "        error = []\n",
    "        for ep in range(episode):\n",
    "            env.reset()\n",
    "            his, reward = env.step()\n",
    "            update = np.zeros((7), dtype = float)\n",
    "            for i in range(len(his)):\n",
    "                state, action, state_reward = his[i]\n",
    "                env.update(state, action, state_reward, reward)\n",
    "\n",
    "            env.value_function += update\n",
    "            error.append(np.sqrt(np.sum(np.power(env.get_value_function()[1:-1] - CFG.target, 2))/5.0))\n",
    "            if (r == 1 and TDplot):\n",
    "                if (ep == 0 or ep == 1 or ep == 10 or ep == 100 or ep == 200 or ep == 299):\n",
    "                    plt.plot(env.get_value_function()[1:-1], label = f'{ep}')\n",
    "        error = np.asarray(error)\n",
    "        total_error += error\n",
    "        if (r == 1 and TDplot):\n",
    "            plt.plot(CFG.target,label = 'true values')\n",
    "            plt.xticks([0,1,2,3,4],['A','B','C','D','E'])\n",
    "            plt.legend()\n",
    "            plt.savefig('figure_6_6/6_6.png')\n",
    "            plt.close()\n",
    "            return\n",
    "    \n",
    "    return total_error/CFG.run\n",
    "\n",
    "\n",
    "# ======================= main ========================================\n",
    "\n",
    "for alpha in CFG.MC_alpha:\n",
    "    error = MC_evaluate(CFG.episode, alpha)\n",
    "    plt.plot(error,'--', label = f'MC - {alpha}')\n",
    "\n",
    "for alpha in CFG.TD_alpha:\n",
    "    error = TD_evaluate(CFG.episode, alpha)\n",
    "    plt.plot(error, label = f'TD - {alpha}')\n",
    "plt.legend()\n",
    "plt.savefig('figure_6_7/6_7.png')\n",
    "plt.close()\n",
    "\n",
    "TD_evaluate(CFG.episode, 0.1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 2/100 [00:15<12:38,  7.74s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-53b20dfdf707>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mfigure7_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-53b20dfdf707>\u001b[0m in \u001b[0;36mfigure7_2\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_STATES\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mtemporal_difference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                     \u001b[0;31m# calculate the RMS error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                     \u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_ind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mTRUE_VALUE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN_STATES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-53b20dfdf707>\u001b[0m in \u001b[0;36mtemporal_difference\u001b[0;34m(value, n, alpha)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# calculate corresponding rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mreturns\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mupdate_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0;31m# add state value to the return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# all states\n",
    "N_STATES = 19\n",
    "\n",
    "# discount\n",
    "GAMMA = 1\n",
    "\n",
    "# all states but terminal states\n",
    "STATES = np.arange(1, N_STATES + 1)\n",
    "\n",
    "# start from the middle state\n",
    "START_STATE = 10\n",
    "\n",
    "# two terminal states\n",
    "# an action leading to the left terminal state has reward -1\n",
    "# an action leading to the right terminal state has reward 1\n",
    "END_STATES = [0, N_STATES + 1]\n",
    "\n",
    "# true state value from bellman equation\n",
    "TRUE_VALUE = np.arange(-20, 22, 2) / 20.0\n",
    "TRUE_VALUE[0] = TRUE_VALUE[-1] = 0\n",
    "\n",
    "# n-steps TD method\n",
    "# @value: values for each state, will be updated\n",
    "# @n: # of steps\n",
    "# @alpha: # step size\n",
    "def temporal_difference(value, n, alpha):\n",
    "    # initial starting state\n",
    "    state = START_STATE\n",
    "\n",
    "    # arrays to store states and rewards for an episode\n",
    "    # space isn't a major consideration, so I didn't use the mod trick\n",
    "    states = [state]\n",
    "    rewards = [0]\n",
    "\n",
    "    # track the time\n",
    "    time = 0\n",
    "\n",
    "    # the length of this episode\n",
    "    T = float('inf')\n",
    "    while True:\n",
    "        # go to next time step\n",
    "        time += 1\n",
    "\n",
    "        if time < T:\n",
    "            # choose an action randomly\n",
    "            if np.random.binomial(1, 0.5) == 1:\n",
    "                next_state = state + 1\n",
    "            else:\n",
    "                next_state = state - 1\n",
    "\n",
    "            if next_state == 0:\n",
    "                reward = -1\n",
    "            elif next_state == 20:\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = 0\n",
    "\n",
    "            # store new state and new reward\n",
    "            states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if next_state in END_STATES:\n",
    "                T = time\n",
    "\n",
    "        # get the time of the state to update\n",
    "        update_time = time - n\n",
    "        if update_time >= 0:\n",
    "            returns = 0.0\n",
    "            # calculate corresponding rewards\n",
    "            for t in range(update_time + 1, min(T, update_time + n) + 1):\n",
    "                returns += pow(GAMMA, t - update_time - 1) * rewards[t]\n",
    "            # add state value to the return\n",
    "            if update_time + n <= T:\n",
    "                returns += pow(GAMMA, n) * value[states[(update_time + n)]]\n",
    "            state_to_update = states[update_time]\n",
    "            # update the state value\n",
    "            if not state_to_update in END_STATES:\n",
    "                value[state_to_update] += alpha * (returns - value[state_to_update])\n",
    "        if update_time == T - 1:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "# Figure 7.2, it will take quite a while\n",
    "def figure7_2():\n",
    "    # all possible steps\n",
    "    steps = np.power(2, np.arange(0, 10))\n",
    "\n",
    "    # all possible alphas\n",
    "    alphas = np.arange(0, 1.1, 0.1)\n",
    "\n",
    "    # each run has 10 episodes\n",
    "    episodes = 10\n",
    "\n",
    "    # perform 100 independent runs\n",
    "    runs = 100\n",
    "\n",
    "    # track the errors for each (step, alpha) combination\n",
    "    errors = np.zeros((len(steps), len(alphas)))\n",
    "    for run in tqdm(range(0, runs)):\n",
    "        for step_ind, step in enumerate(steps):\n",
    "            for alpha_ind, alpha in enumerate(alphas):\n",
    "                # print('run:', run, 'step:', step, 'alpha:', alpha)\n",
    "                value = np.zeros(N_STATES + 2)\n",
    "                for ep in range(0, episodes):\n",
    "                    temporal_difference(value, step, alpha)\n",
    "                    # calculate the RMS error\n",
    "                    errors[step_ind, alpha_ind] += np.sqrt(np.sum(np.power(value - TRUE_VALUE, 2)) / N_STATES)\n",
    "    # take average\n",
    "    errors /= episodes * runs\n",
    "\n",
    "    print(errors)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    figure7_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
